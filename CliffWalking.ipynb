{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd42f02",
   "metadata": {},
   "source": [
    "# Cliff Walking\n",
    "## Project Overview\n",
    "This project implements Q-Learning and SARSA, two fundamental reinforcement learning algorithms, to solve the Cliff Walking problem. The Cliff Walking environment is a 4x12 grid where an agent must navigate from a starting point (3,0) to a goal (3,11) while avoiding cliffs located at row 3, columns 1 to 10. Falling into the cliff incurs a heavy penalty (-100), each step costs -1, and reaching the goal yields a reward of 0. The code is structured into five cells for modularity, with detailed comments explaining each part. The output visualizes the optimal policies for both algorithms, highlighting their differences (Q-Learning tends to take riskier paths near the cliff, while SARSA prefers safer routes due to its on-policy nature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82026fe",
   "metadata": {},
   "source": [
    "## Import Libraries and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab80ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports necessary libraries\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f84f7",
   "metadata": {},
   "source": [
    "## defines the CliffWalkingEnv class,\n",
    "### which represents the Cliff Walking environment, a 4x12 grid where the agent must navigate\n",
    "### from start (3,0) to goal (3,11) while avoiding cliffs (row 3, columns 1-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0478eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    def __init__(self):\n",
    "        self.rows = 4  # Grid height\n",
    "        self.cols = 12  # Grid width\n",
    "        self.start = (3, 0)  # Starting position\n",
    "        self.goal = (3, 11)  # Goal position\n",
    "        self.cliff = [(3, i) for i in range(1, 11)]  # Cliff positions\n",
    "        self.actions = ['up', 'right', 'down', 'left']  # Possible actions\n",
    "        self.action_to_index = {a: i for i, a in enumerate(self.actions)}  # Action mapping\n",
    "        self.state = self.start  # Current state\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets the environment to the starting state\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Takes an action, updates the state, and returns next state, reward, and done flag\n",
    "        row, col = self.state\n",
    "        if action == 'up':\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 'right':\n",
    "            col = min(col + 1, self.cols - 1)\n",
    "        elif action == 'down':\n",
    "            row = min(row + 1, self.rows - 1)\n",
    "        elif action == 'left':\n",
    "            col = max(col - 1, 0)\n",
    "\n",
    "        next_state = (row, col)\n",
    "        reward = -1  # Default step penalty\n",
    "        done = False\n",
    "\n",
    "        if next_state in self.cliff:\n",
    "            reward = -100  # Penalty for falling into the cliff\n",
    "            next_state = self.start  # Return to start\n",
    "        elif next_state == self.goal:\n",
    "            reward = 0  # Reward for reaching the goal\n",
    "            done = True\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab7266",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "### This cell implements the Q-Learning algorithm, an off-policy reinforcement\n",
    "### learning method that updates Q-values based on the maximum Q-value of the next state.\n",
    "### It uses an epsilon-greedy policy for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49303d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    # Initialize Q-table and reward tracking\n",
    "    q_table = np.zeros((env.rows, env.cols, len(env.actions)))\n",
    "    episode_rewards = []  # Store total reward per episode\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(env.actions)  # Explore\n",
    "            else:\n",
    "                action = env.actions[np.argmax(q_table[state[0], state[1]])]  # Exploit\n",
    "            \n",
    "            action_idx = env.action_to_index[action]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-value update rule\n",
    "            q_table[state[0], state[1], action_idx] += alpha * (\n",
    "                reward + gamma * np.max(q_table[next_state[0], next_state[1]]) -\n",
    "                q_table[state[0], state[1], action_idx]\n",
    "            )\n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return q_table, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0a25c",
   "metadata": {},
   "source": [
    "# SARSA Algorithm\n",
    "### Description: This cell implements the SARSA algorithm, an on-policy reinforcement\n",
    "### learning method that updates Q-values based on the action chosen by the same policy\n",
    "### used for exploration (epsilon-greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa8e8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    # Initialize Q-table and reward tracking\n",
    "    q_table = np.zeros((env.rows, env.cols, len(env.actions)))\n",
    "    episode_rewards = []  # Store total reward per episode\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        # Choose initial action\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(env.actions)\n",
    "        else:\n",
    "            action = env.actions[np.argmax(q_table[state[0], state[1]])]\n",
    "        \n",
    "        while True:\n",
    "            action_idx = env.action_to_index[action]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Choose next action\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                next_action = random.choice(env.actions)\n",
    "            else:\n",
    "                next_action = env.actions[np.argmax(q_table[next_state[0], next_state[1]])]\n",
    "            \n",
    "            next_action_idx = env.action_to_index[next_action]\n",
    "            \n",
    "            # Q-value update rule\n",
    "            q_table[state[0], state[1], action_idx] += alpha * (\n",
    "                reward + gamma * q_table[next_state[0], next_state[1], next_action_idx] -\n",
    "                q_table[state[0], state[1], action_idx]\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return q_table, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f1237",
   "metadata": {},
   "source": [
    "# Policy Visualization\n",
    "### Description: This cell defines a function to visualize the optimal policy derived\n",
    "### from the Q-table. It displays the grid with actions (U, R, D, L), cliffs (C), and\n",
    "### the goal (G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46a9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(q_table, env):\n",
    "    policy = np.chararray((env.rows, env.cols), unicode=True)\n",
    "    for i in range(env.rows):\n",
    "        for j in range(env.cols):\n",
    "            if (i, j) in env.cliff:\n",
    "                policy[i, j] = 'C'  # Cliff\n",
    "            elif (i, j) == env.goal:\n",
    "                policy[i, j] = 'G'  # Goal\n",
    "            else:\n",
    "                best_action = np.argmax(q_table[i, j])\n",
    "                policy[i, j] = env.actions[best_action][0].upper()  # Action\n",
    "    print(\"\\nPolicy Grid:\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc528e7",
   "metadata": {},
   "source": [
    "# Reward Visualization Chart\n",
    "### Description: Generates a line chart to compare the average reward per\n",
    "### episode for\n",
    "### Q-Learning and SARSA, showing how their performance improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99eb81fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'line',\n",
       " 'data': {'labels': [],\n",
       "  'datasets': [{'label': 'Q-Learning Rewards',\n",
       "    'data': [],\n",
       "    'borderColor': '#1f77b4',\n",
       "    'backgroundColor': 'rgba(31, 119, 180, 0.1)',\n",
       "    'fill': True,\n",
       "    'tension': 0.4},\n",
       "   {'label': 'SARSA Rewards',\n",
       "    'data': [],\n",
       "    'borderColor': '#ff7f0e',\n",
       "    'backgroundColor': 'rgba(255, 127, 14, 0.1)',\n",
       "    'fill': True,\n",
       "    'tension': 0.4}]},\n",
       " 'options': {'responsive': True,\n",
       "  'plugins': {'title': {'display': True,\n",
       "    'text': 'Average Reward per Episode (Q-Learning vs SARSA)',\n",
       "    'font': {'size': 16}},\n",
       "   'legend': {'position': 'top'}},\n",
       "  'scales': {'x': {'title': {'display': True, 'text': 'Episode'}},\n",
       "   'y': {'title': {'display': True, 'text': 'Average Reward'},\n",
       "    'suggestedMin': -100,\n",
       "    'suggestedMax': 0}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charts Js\n",
    "{\n",
    "  \"type\": \"line\",\n",
    "  \"data\": {\n",
    "    \"labels\": [],  # Will be populated with episode numbers\n",
    "    \"datasets\": [\n",
    "      {\n",
    "        \"label\": \"Q-Learning Rewards\",\n",
    "        \"data\": [],  # Will be populated with Q-Learning rewards\n",
    "        \"borderColor\": \"#1f77b4\",  # Blue for Q-Learning\n",
    "        \"backgroundColor\": \"rgba(31, 119, 180, 0.1)\",\n",
    "        \"fill\": True,\n",
    "        \"tension\": 0.4\n",
    "      },\n",
    "      {\n",
    "        \"label\": \"SARSA Rewards\",\n",
    "        \"data\": [],  # Will be populated with SARSA rewards\n",
    "        \"borderColor\": \"#ff7f0e\",  # Orange for SARSA\n",
    "        \"backgroundColor\": \"rgba(255, 127, 14, 0.1)\",\n",
    "        \"fill\": True,\n",
    "        \"tension\": 0.4\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"options\": {\n",
    "    \"responsive\": True,\n",
    "    \"plugins\": {\n",
    "      \"title\": {\n",
    "        \"display\": True,\n",
    "        \"text\": \"Average Reward per Episode (Q-Learning vs SARSA)\",\n",
    "        \"font\": { \"size\": 16 }\n",
    "      },\n",
    "      \"legend\": { \"position\": \"top\" }\n",
    "    },\n",
    "    \"scales\": {\n",
    "      \"x\": {\n",
    "        \"title\": { \"display\": True, \"text\": \"Episode\" }\n",
    "      },\n",
    "      \"y\": {\n",
    "        \"title\": { \"display\": True, \"text\": \"Average Reward\" },\n",
    "        \"suggestedMin\": -100,\n",
    "        \"suggestedMax\": 0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c562ddb",
   "metadata": {},
   "source": [
    "# Main Execution\n",
    "### Description: This cell initializes the environment, runs both Q-Learning and SARSA\n",
    "### algorithms, and prints their resulting policies for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2661a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Q-Learning...\n",
      "\n",
      "Policy Grid:\n",
      "[['R' 'D' 'R' 'R' 'L' 'U' 'L' 'D' 'D' 'D' 'R' 'D']\n",
      " ['U' 'R' 'U' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'D']\n",
      " ['U' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'G']]\n",
      "\n",
      "Running SARSA...\n",
      "\n",
      "Policy Grid:\n",
      "[['R' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'U' 'L' 'U' 'U' 'U' 'U' 'U' 'R' 'U' 'R' 'D']\n",
      " ['U' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'G']]\n",
      "\n",
      "Chart data prepared for visualization (populates the line chart above).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = CliffWalkingEnv()\n",
    "    episodes = 500\n",
    "    \n",
    "    # Run Q-Learning\n",
    "    print(\"Running Q-Learning...\")\n",
    "    q_table_ql, rewards_ql = q_learning(env, episodes=episodes, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    print_policy(q_table_ql, env)\n",
    "    \n",
    "    # Run SARSA\n",
    "    print(\"\\nRunning SARSA...\")\n",
    "    q_table_sarsa, rewards_sarsa = sarsa(env, episodes=episodes, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    print_policy(q_table_sarsa, env)\n",
    "    \n",
    "    # Prepare chart data\n",
    "    chart_data = {\n",
    "        \"labels\": list(range(1, episodes + 1)),  # Episode numbers\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"label\": \"Q-Learning Rewards\",\n",
    "                \"data\": rewards_ql,  # Q-Learning rewards\n",
    "                \"borderColor\": \"#1f77b4\",\n",
    "                \"backgroundColor\": \"rgba(31, 119, 180, 0.1)\",\n",
    "                \"fill\": True,\n",
    "                \"tension\": 0.4\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"SARSA Rewards\",\n",
    "                \"data\": rewards_sarsa,  # SARSA rewards\n",
    "                \"borderColor\": \"#ff7f0e\",\n",
    "                \"backgroundColor\": \"rgba(255, 127, 14, 0.1)\",\n",
    "                \"fill\": True,\n",
    "                \"tension\": 0.4\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    print(\"\\nChart data prepared for visualization (populates the line chart above).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87174da",
   "metadata": {},
   "source": [
    "Cell-by-Cell Explanation\n",
    "\n",
    "Cell 1: Environment Setup\n",
    "\n",
    "Defines the CliffWalkingEnv class, representing the 4x12 grid.\n",
    "Includes methods for resetting (reset) and stepping (step) through the environment.\n",
    "Rewards: -1 per step, -100 for falling into the cliff, 0 for reaching the goal.\n",
    "\n",
    "\n",
    "Cell 2: Q-Learning Algorithm\n",
    "\n",
    "Implements Q-Learning with reward tracking to store the total reward per episode.\n",
    "Uses an $ \\epsilon $-greedy policy ($ \\epsilon = 0.1 $) for exploration.\n",
    "Parameters: learning rate ($ \\alpha = 0.1 $), discount factor ($ \\gamma = 0.99 $), 500 episodes.\n",
    "\n",
    "\n",
    "Cell 3: SARSA Algorithm\n",
    "\n",
    "Implements SARSA with reward tracking, updating Q-values based on the next action chosen by the same policy.\n",
    "Same parameters as Q-Learning for fair comparison.\n",
    "\n",
    "\n",
    "Cell 4: Policy Visualization\n",
    "\n",
    "Displays the optimal policy as a grid with actions (U, R, D, L), cliffs (C), and the goal (G).\n",
    "\n",
    "\n",
    "Cell 5: Reward Visualization Chart\n",
    "\n",
    "Defines a Chart.js line chart to plot average rewards per episode for Q-Learning and SARSA.\n",
    "Uses distinct colors (blue for Q-Learning, orange for SARSA) and a smooth line style.\n",
    "The chart is populated in Cell 6 with episode numbers and reward data.\n",
    "\n",
    "\n",
    "Cell 6: Main Execution\n",
    "\n",
    "Runs both algorithms, prints their policies, and prepares data for the reward chart.\n",
    "The chart data is printed as a confirmation, but in a real environment, it populates the Chart.js widget.\n",
    "\n",
    "\n",
    "\n",
    "Chart Details\n",
    "\n",
    "Type: Line chart (smooth curves with filled areas).\n",
    "Data: Average reward per episode for Q-Learning and SARSA.\n",
    "X-Axis: Episode numbers (1 to 500).\n",
    "Y-Axis: Reward values (typically from -100 to 0, reflecting step penalties and cliff falls).\n",
    "Purpose: Shows how quickly each algorithm learns to avoid cliffs and reach the goal, with Q-Learning often converging faster but SARSA being more stable due to its safer paths.\n",
    "\n",
    "How to Run\n",
    "\n",
    "Requirements: Python with numpy and random. The chart requires a Chart.js-compatible environment (e.g., a web-based Jupyter extension or a custom UI).\n",
    "Execution: Run the code in a Python environment. The chart data is prepared in Cell 6 and can be visualized in a Chart.js-compatible interface.\n",
    "Output: Policy grids for both algorithms and a line chart comparing their reward trends.\n",
    "\n",
    "Notes\n",
    "\n",
    "The chart helps visualize that Q-Learning may achieve higher rewards faster (riskier paths), while SARSAâ€™s rewards improve more steadily (safer paths).\n",
    "To experiment, try adjusting $ \\alpha $, $ \\epsilon $, or the number of episodes and observe changes in the chart.\n",
    "If you want additional visualizations (e.g., learning rate impact, grid path animation), or a different environment (e.g., Frozen Lake), let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
