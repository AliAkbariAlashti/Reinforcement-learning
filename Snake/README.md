# ğŸ Snake Game with Q-Learning

This project implements a classic Snake game powered by a simple **Q-Learning** algorithm. The snake learns to reach food while avoiding obstacles and its own body using basic reinforcement learning techniques. The game is built with **Python** and **Pygame**.

---

## ğŸ¯ Features

- ğŸ§± **Environment:**  
  20x20 grid-based world with static obstacles, food, and the moving snake.

- ğŸ§  **Q-Learning Logic:**
  - **State:**  
    Relative position of food (x, y) and danger indicators (up, down, left, right).
  - **Actions:**  
    `Up`, `Down`, `Left`, `Right`.
  - **Rewards:**  
    +10 for eating food, -10 for collisions, -0.1 for each movement step.
  - **Hyperparameters:**  
    - Learning Rate `Î± = 0.1`  
    - Discount Factor `Î³ = 0.9`  
    - Exploration Rate `Îµ = 0.1`

- ğŸ¨ **Graphics:**
  - Snake: ğŸŸ© Green  
  - Food: ğŸŸ¥ Red  
  - Obstacles: ğŸŸ¦ Blue  
  - Score & Episode counter displayed on screen

- ğŸ•¹ï¸ **Execution:**  
  Runs with a standard `pygame` loop. No external reinforcement learning libraries required.

---

## ğŸ§° Requirements

- âœ… Python 3.8+
- âœ… [Pygame](https://www.pygame.org/) library

ğŸš€ How to Run
Download the maze_qlearning.py file.

Run the script locally with:
python maze_qlearning.py
ğŸ“ˆ Observing the Learning Process
ğŸ”„ Early Episodes:
-The agent explores randomly and often hits walls.

ğŸ§  Later Episodes:
The agent gradually learns to find shorter and safer paths to the goal.

ğŸ“Š On-Screen Info:
  - The score, current episode, and number of steps are displayed in the top-left corner.

ğŸ’¡ Possible Improvements
  - ğŸ“‰ Gradually decrease Îµ over time to shift from exploration to exploitation.

  - ğŸ§± Introduce dynamic/moving walls for a more challenging environment.

  - ğŸ¤– Implement Deep Q-Learning (DQN) for scaling to larger maze sizes.

-ğŸ§­ Store and visualize the optimal path after training is complete.
